{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "So far we have looked at how we can work with numerical data in performing different types of classification.\n",
    "\n",
    "Today we look at how machine learning can be used to process text. This is generally a field of machine learning called Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nlp.png\" height=\"32\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing has many applications such as:\n",
    "    - Translation (E.g. Google translate)\n",
    "    - Summarization\n",
    "    - Information Retrieval: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/obama.png\" height=\"32\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sentiment_analysis.png\" height=\"42\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/googlenews.png\" height=\"32\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into an example, let's look at how text is preprocessed before being fed into a machine learning model\n",
    "\n",
    "\n",
    "## Text Preprocessing\n",
    "Text preprocessing generally involves a number of steps:\n",
    "1. Cleaning: Removing special characters etc\n",
    "2. Tokenizing. A token is a string of contiguous characters. A word is a token\n",
    "3. Vectorizing: Moving from words to vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one.',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'tweet.',\n",
       " 'tweet?']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This is the first tweet.', 'This is the second second tweet.','And the third one.','Is this the first tweet?'] \n",
    "#vocab = [word.lower() for tweet in tweets for word in tweet.split() ]\n",
    "vocab = []\n",
    "for tweet in tweets:\n",
    "\tfor word in tweet.split():\n",
    "\t\tvocab.append(word.lower())\n",
    "        \n",
    "vocab = list(set(vocab))\n",
    "#vocab\n",
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "\n",
    "### CountVectorizer\n",
    "\n",
    "Count Vectorizer is useful for tokenizing. In Sklearn tokenizing strings gives an integer id for each possible token, for instance by using white-spaces and punctuation as token separators. CountVectorizer counts the number of token occurrences i.e. the number of times a token appears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'first', 'is', 'one', 'second', 'the', 'third', 'this', 'tweet']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "features = vectorizer.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 2, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  transform \n",
    ".transform changes the text to a list ot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Lets assume this is a new tweet']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Grams and Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'first', 'first tweet', 'is', 'is the', 'is this', 'one', 'second', 'second second', 'second tweet', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 'this the', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "\n",
    "X = bigram_vectorizer.fit_transform(tweets)\n",
    "features_bi = bigram_vectorizer.get_feature_names()\n",
    "print(sorted(features_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'and the third', 'first', 'first tweet', 'is', 'is the', 'is the first', 'is the second', 'is this', 'is this the', 'one', 'second', 'second second', 'second second tweet', 'second tweet', 'the', 'the first', 'the first tweet', 'the second', 'the second second', 'the third', 'the third one', 'third', 'third one', 'this', 'this is', 'this is the', 'this the', 'this the first', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3),  min_df=1)\n",
    "\n",
    "X = trigram_vectorizer.fit_transform(tweets)\n",
    "features_tri = trigram_vectorizer.get_feature_names()\n",
    "print(sorted(features_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems:\n",
    "1. Longer documents will have higher average count values than shorter documents\n",
    "2. Some words are very common e.g. 'the', 'and', 'is' will automatically have higher counts\n",
    "\n",
    "#### Solution:\n",
    "1. Term Frequencies times Inverse Document Frequency\n",
    "\n",
    "### TF-IDF  \n",
    "Term frequency is the number of times a word appears in a document, or in this case a tweet. If the word \"kenya\" appears twice in a the tweet \"\", then the term frequency of kenya is 2.\n",
    "\n",
    "Term Frequency can be taken as the raw count of a term but is often adjusted to account for the total number of words in the document. Example: Instead of -the word 'kenya' was used 2 times, TF says the word 'kenya' was used 20% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tf.png\" height=\"32\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take care of the second problem, we use IDF.\n",
    "Inverse Document Frequency factor is a way of diminishing the weight of terms that occur very frequently in a document set and increasing the weight of terms that occur rarely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/idf.png\" height=\"42\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 31)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will look at text processing. The overall goal will be to predict who has written some tweets.\n",
    "\n",
    "\n",
    "# Author Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Importations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "nrs = pd.read_csv('datasets/kenya.csv')\n",
    "nrs = shuffle(nrs)\n",
    "nrs.head()\n",
    "\n",
    "#Match the indices to target names\n",
    "target_names = ['UKenyatta','RailaOdinga']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Split the data in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = nrs['tweet']\n",
    "\n",
    "y = nrs['class']\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "X_train = X[:-1000]\n",
    "y_train = y[:-1000]\n",
    "\n",
    "X_test = X[-1000:]\n",
    "y_test = y[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Create and Train a classifier\n",
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47840, 14776)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Occurences\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47840, 14776)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequencies\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training a classifier\n",
    "classifier = LogisticRegression()\n",
    "clf = classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model is:\n",
      "0.991\n",
      "Confusion matrix:\n",
      "[[304   9]\n",
      " [  0 687]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "X_tests_counts = count_vect.transform(X_test)\n",
    "X_tests_tfidf = tfidf_transformer.transform(X_tests_counts)\n",
    "expected  = y_test\n",
    "predicted = clf.predict(X_tests_tfidf)\n",
    "print(\"Accuracy of our model is:\\n%s\" % metrics.accuracy_score(expected, predicted))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Four years ago, we made a commitment to Kenyans. We are proud that our record speaks for itself.' ===> UKenyatta\n",
      "\n",
      "\"Kenyans don't want handouts. They need a hand up initiative from their government. The economy must grow for all - not the few.\" ===> RailaOdinga\n",
      "\n",
      "'After a successful tour of Narok county, we conclude our visit with a major rally today in Narok town.' ===> RailaOdinga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "#Predicting Outcome\n",
    "tweet1 = 'Four years ago, we made a commitment to Kenyans. We are proud that our record speaks for itself.'\n",
    "tweet2 = 'Kenyans don\\'t want handouts. They need a hand up initiative from their government. The economy must grow for all - not the few.'\n",
    "tweet3 = 'After a successful tour of Narok county, we conclude our visit with a major rally today in Narok town.'\n",
    "tweets_new = [tweet1,tweet2,tweet3]\n",
    "X_new_counts = count_vect.transform(tweets_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for tw, category in zip(tweets_new, predicted):\n",
    "    print('\\n%r ===> %s' % (tw, my_data['target_names'][category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
