{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "So far we have looked at how we can work with numerical data in performing different types of classification.\n",
    "\n",
    "Today we look at how machine learning can be used to process text. This is generally a field of machine learning called Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvesting Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy # https://github.com/tweepy/tweepy\n",
    "import csv # Write csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API credentials\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_key = ''\n",
    "access_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = ['BarackObama', 'realDonaldTrump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "     \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []  \n",
    "     \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "     \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "     \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "     \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"\\tGetting tweets before %s\" % (oldest))\n",
    "         \n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "         \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "         \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "         \n",
    "        print(\"\\t...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "        # transform the tweepy tweets into a 2D array that will populate the csv\n",
    "        outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "                 \n",
    "        #write the csv\n",
    "        with open('datasets/{}_tweets.csv'.format(screen_name), 'a') as f:\n",
    "                writer = csv.writer(f)\n",
    "                #writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "                writer.writerows(outtweets)\n",
    "                #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting @BarackObama's tweets\n",
      "\n",
      "\tGetting tweets before 912724426709503999\n",
      "\t...400 tweets downloaded so far\n",
      "\tGetting tweets before 776160016483033087\n",
      "\t...600 tweets downloaded so far\n",
      "\tGetting tweets before 748957408878211071\n",
      "\t...800 tweets downloaded so far\n",
      "\tGetting tweets before 726521029401665535\n",
      "\t...1000 tweets downloaded so far\n",
      "\tGetting tweets before 705160499902726144\n",
      "\t...1200 tweets downloaded so far\n",
      "\tGetting tweets before 687099389483958271\n",
      "\t...1400 tweets downloaded so far\n",
      "\tGetting tweets before 668933428620857346\n",
      "\t...1600 tweets downloaded so far\n",
      "\tGetting tweets before 648539602848886784\n",
      "\t...1800 tweets downloaded so far\n",
      "\tGetting tweets before 628965547690954751\n",
      "\t...2000 tweets downloaded so far\n",
      "\tGetting tweets before 616281266975956991\n",
      "\t...2200 tweets downloaded so far\n",
      "\tGetting tweets before 598921453338136575\n",
      "\t...2400 tweets downloaded so far\n",
      "\tGetting tweets before 581193144181604351\n",
      "\t...2599 tweets downloaded so far\n",
      "\tGetting tweets before 560847716578623487\n",
      "\t...2799 tweets downloaded so far\n",
      "\tGetting tweets before 546059864611377151\n",
      "\t...2997 tweets downloaded so far\n",
      "\tGetting tweets before 533440437613514752\n",
      "\t...3196 tweets downloaded so far\n",
      "\tGetting tweets before 517744919772155903\n",
      "\t...3198 tweets downloaded so far\n",
      "\tGetting tweets before 517743546116214783\n",
      "\t...3198 tweets downloaded so far\n",
      "\n",
      "Getting @realDonaldTrump's tweets\n",
      "\n",
      "\tGetting tweets before 1141164726593998849\n",
      "\t...400 tweets downloaded so far\n",
      "\tGetting tweets before 1137568536690343935\n",
      "\t...600 tweets downloaded so far\n",
      "\tGetting tweets before 1132381698568663044\n",
      "\t...800 tweets downloaded so far\n",
      "\tGetting tweets before 1128246369674829831\n",
      "\t...999 tweets downloaded so far\n",
      "\tGetting tweets before 1125929401223454719\n",
      "\t...1196 tweets downloaded so far\n",
      "\tGetting tweets before 1122334000519868415\n",
      "\t...1396 tweets downloaded so far\n",
      "\tGetting tweets before 1117909721766174719\n",
      "\t...1596 tweets downloaded so far\n",
      "\tGetting tweets before 1111768933651152896\n",
      "\t...1795 tweets downloaded so far\n",
      "\tGetting tweets before 1105838130685493247\n",
      "\t...1995 tweets downloaded so far\n",
      "\tGetting tweets before 1100584141274398719\n",
      "\t...2194 tweets downloaded so far\n",
      "\tGetting tweets before 1092901071910043647\n",
      "\t...2394 tweets downloaded so far\n",
      "\tGetting tweets before 1086252588088082431\n",
      "\t...2594 tweets downloaded so far\n",
      "\tGetting tweets before 1079121410935660543\n",
      "\t...2794 tweets downloaded so far\n",
      "\tGetting tweets before 1071159669949911043\n",
      "\t...2989 tweets downloaded so far\n",
      "\tGetting tweets before 1063081553075625983\n",
      "\t...3189 tweets downloaded so far\n",
      "\tGetting tweets before 1057683481793302527\n",
      "\t...3189 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    " for username in usernames:\n",
    "            try:\n",
    "                    #pass in the username of the account you want to download\n",
    "                    print(\"\\nGetting @{}'s tweets\\n\".format(username))\n",
    "                    get_all_tweets(username)\n",
    "            except:\n",
    "                    print('\\tError! Failed to fetch tweets for {}'.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Author Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = pd.read_csv('datasets/{}_tweets.csv'.format(usernames[0]))\n",
    "user2 = pd.read_csv('datasets/{}_tweets.csv'.format(usernames[1]))\n",
    "\n",
    "# Assign columns to the data\n",
    "user1.columns = ['id', 'timestamp', 'tweet']\n",
    "user2.columns = ['id', 'timestamp', 'tweet']\n",
    "\n",
    "# Create target columns for the users \n",
    "user1['Name'] = 0\n",
    "user2['Name'] = 1\n",
    "\n",
    "# Join the two dataframes into one huge dataframe and shuffle them\n",
    "collectiveTweets = pd.concat([user1, user2])\n",
    "collectiveTweets = shuffle(collectiveTweets)\n",
    "\n",
    "# Target names\n",
    "target_names = [usernames[0], usernames[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Split the data in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = collectiveTweets['tweet']\n",
    "y = collectiveTweets['Name']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "X_train = X[:-1000]\n",
    "y_train = y[:-1000]\n",
    "\n",
    "X_test = X[-1000:]\n",
    "y_test = y[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Create and Train a classifier\n",
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62505, 14643)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Occurences\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62505, 14643)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequencies\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a classifier\n",
    "classifier = LogisticRegression()\n",
    "clf = classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model is:\n",
      "0.999\n",
      "Confusion matrix:\n",
      "[[540   0]\n",
      " [  1 459]]\n"
     ]
    }
   ],
   "source": [
    "X_tests_counts = count_vect.transform(X_test)\n",
    "X_tests_tfidf = tfidf_transformer.transform(X_tests_counts)\n",
    "expected  = y_test\n",
    "predicted = clf.predict(X_tests_tfidf)\n",
    "print(\"Accuracy of our model is:\\n%s\" % metrics.accuracy_score(expected, predicted))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Food ===> BarackObama\n",
      "\n",
      "Machine Learning ===> realDonaldTrump\n",
      "\n",
      "Music ===> realDonaldTrump\n",
      "\n",
      "yes we can ===> BarackObama\n",
      "\n",
      "I love you ===> BarackObama\n",
      "\n",
      "Go to hell ===> BarackObama\n",
      "\n",
      "Yaaay ===> realDonaldTrump\n",
      "\n",
      "Nice ===> realDonaldTrump\n",
      "\n",
      "God bless America ===> realDonaldTrump\n",
      "\n",
      "billions ===> realDonaldTrump\n"
     ]
    }
   ],
   "source": [
    "#Predicting Outcome\n",
    "tweet1 = 'Food'\n",
    "tweet2 = 'Machine Learning'\n",
    "tweet3 = 'Music'\n",
    "tweet4 = 'yes we can'\n",
    "tweet5 = 'I love you'\n",
    "tweet6 = 'Go to hell'\n",
    "tweet7 = 'Yaaay'\n",
    "tweet8 = 'Nice'\n",
    "tweet9 = 'God bless America'\n",
    "tweet10 = 'billions'\n",
    "\n",
    "tweets_new = [tweet1, tweet2, tweet3, tweet4, tweet5, tweet6, tweet7, tweet8, tweet9, tweet10]\n",
    "X_new_counts = count_vect.transform(tweets_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for tw, category in zip(tweets_new, predicted):\n",
    "    print('\\n{} ===> {}'.format(tw, target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
