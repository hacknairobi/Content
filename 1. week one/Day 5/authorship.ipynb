{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "So far we have looked at how we can work with numerical data in performing different types of classification.\n",
    "\n",
    "Today we look at how machine learning can be used to process text. This is generally a field of machine learning called Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nlp.png\" height=\"32\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing has many applications such as:\n",
    "    - Translation (E.g. Google translate)\n",
    "    - Summarization\n",
    "    - Information Retrieval: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/obama.png\" height=\"32\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sentiment_analysis.png\" height=\"42\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/googlenews.png\" height=\"32\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into an example, let's look at how text is preprocessed before being fed into a machine learning model\n",
    "\n",
    "\n",
    "## Text Preprocessing\n",
    "Text preprocessing generally involves a number of steps:\n",
    "1. Cleaning: Removing special characters etc\n",
    "2. Tokenizing. A token is a string of contiguous characters. A word is a token\n",
    "3. Vectorizing: Moving from words to vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one.',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'tweet.',\n",
       " 'tweet?']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['This is the first tweet.', 'This is the second second tweet.','And the third one.','Is this the first tweet?'] \n",
    "#vocab = [word.lower() for tweet in tweets for word in tweet.split() ]\n",
    "vocab = []\n",
    "for tweet in tweets:\n",
    "\tfor word in tweet.split():\n",
    "\t\tvocab.append(word.lower())\n",
    "        \n",
    "vocab = list(set(vocab))\n",
    "#vocab\n",
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "\n",
    "### CountVectorizer\n",
    "\n",
    "Count Vectorizer is useful for tokenizing. In Sklearn tokenizing strings gives an integer id for each possible token, for instance by using white-spaces and punctuation as token separators. CountVectorizer counts the number of token occurrences i.e. the number of times a token appears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'first', 'is', 'one', 'second', 'the', 'third', 'this', 'tweet']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "features = vectorizer.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 2, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  transform \n",
    ".transform changes the text to a list ot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Lets assume this is a new tweet']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Grams and Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'first', 'first tweet', 'is', 'is the', 'is this', 'one', 'second', 'second second', 'second tweet', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 'this the', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "\n",
    "X = bigram_vectorizer.fit_transform(tweets)\n",
    "features_bi = bigram_vectorizer.get_feature_names()\n",
    "print(sorted(features_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'and the third', 'first', 'first tweet', 'is', 'is the', 'is the first', 'is the second', 'is this', 'is this the', 'one', 'second', 'second second', 'second second tweet', 'second tweet', 'the', 'the first', 'the first tweet', 'the second', 'the second second', 'the third', 'the third one', 'third', 'third one', 'this', 'this is', 'this is the', 'this the', 'this the first', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3),  min_df=1)\n",
    "\n",
    "X = trigram_vectorizer.fit_transform(tweets)\n",
    "features_tri = trigram_vectorizer.get_feature_names()\n",
    "print(sorted(features_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems:\n",
    "1. Longer documents will have higher average count values than shorter documents\n",
    "2. Some words are very common e.g. 'the', 'and', 'is' will automatically have higher counts\n",
    "\n",
    "#### Solution:\n",
    "1. Term Frequencies times Inverse Document Frequency\n",
    "\n",
    "### TF-IDF  \n",
    "Term frequency is the number of times a word appears in a document, or in this case a tweet. If the word \"kenya\" appears twice in a the tweet \"\", then the term frequency of kenya is 2.\n",
    "\n",
    "Term Frequency can be taken as the raw count of a term but is often adjusted to account for the total number of words in the document. Example: Instead of -the word 'kenya' was used 2 times, TF says the word 'kenya' was used 20% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tf.png\" height=\"32\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take care of the second problem, we use IDF.\n",
    "Inverse Document Frequency factor is a way of diminishing the weight of terms that occur very frequently in a document set and increasing the weight of terms that occur rarely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/idf.png\" height=\"42\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 31)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will look at text processing. The overall goal will be to predict who has written some tweets.\n",
    "\n",
    "\n",
    "# Author Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8536</th>\n",
       "      <td>1055716406564716546</td>\n",
       "      <td>2018-10-26 07:03:00</td>\n",
       "      <td>b'@mkmuigai @Migwination Na Selina Gomez'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>1054967240322355201</td>\n",
       "      <td>2018-10-24 05:26:05</td>\n",
       "      <td>b\"@Mwihaki_ @kuirab You're not the only one. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>1031474152790736903</td>\n",
       "      <td>2018-08-20 09:32:56</td>\n",
       "      <td>b\"RT @gitweeta: Kenya's geography according to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>1013369290844770304</td>\n",
       "      <td>2018-07-01 10:30:41</td>\n",
       "      <td>b'RT @Mr_DrinksOnMe: The pattern on these shoe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9154</th>\n",
       "      <td>918136202108198913</td>\n",
       "      <td>2017-10-11 15:28:23</td>\n",
       "      <td>b\"RT @iam_bett: There's life away from Raila O...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id            timestamp  \\\n",
       "8536  1055716406564716546  2018-10-26 07:03:00   \n",
       "6936  1054967240322355201  2018-10-24 05:26:05   \n",
       "3276  1031474152790736903  2018-08-20 09:32:56   \n",
       "2449  1013369290844770304  2018-07-01 10:30:41   \n",
       "9154   918136202108198913  2017-10-11 15:28:23   \n",
       "\n",
       "                                                  tweet  Name  \n",
       "8536          b'@mkmuigai @Migwination Na Selina Gomez'     1  \n",
       "6936  b\"@Mwihaki_ @kuirab You're not the only one. I...     1  \n",
       "3276  b\"RT @gitweeta: Kenya's geography according to...     1  \n",
       "2449  b'RT @Mr_DrinksOnMe: The pattern on these shoe...     1  \n",
       "9154  b\"RT @iam_bett: There's life away from Raila O...     1  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1 = pd.read_csv('datasets/Miss_Wanza_tweets.csv')\n",
    "user1.columns = ['id', 'timestamp', 'tweet']\n",
    "user2 = pd.read_csv('datasets/LewisMunyi_tweets.csv')\n",
    "user2.columns = ['id', 'timestamp', 'tweet']\n",
    "user1['Name'] = 0\n",
    "user2['Name'] = 1\n",
    "collectiveTweets = pd.concat([user1, user2])\n",
    "collectiveTweets = shuffle(collectiveTweets)\n",
    "target_names = ['Rosianah','Lewis']\n",
    "collectiveTweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Split the data in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = collectiveTweets['tweet']\n",
    "y = collectiveTweets['Name']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "X_train = X[:-1000]\n",
    "y_train = y[:-1000]\n",
    "\n",
    "X_test = X[-1000:]\n",
    "y_test = y[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Create and Train a classifier\n",
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12951, 8878)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Occurences\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12951, 8878)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequencies\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a classifier\n",
    "classifier = LogisticRegression()\n",
    "clf = classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model is:\n",
      "0.975\n",
      "Confusion matrix:\n",
      "[[144  24]\n",
      " [  1 831]]\n"
     ]
    }
   ],
   "source": [
    "X_tests_counts = count_vect.transform(X_test)\n",
    "X_tests_tfidf = tfidf_transformer.transform(X_tests_counts)\n",
    "expected  = y_test\n",
    "predicted = clf.predict(X_tests_tfidf)\n",
    "print(\"Accuracy of our model is:\\n%s\" % metrics.accuracy_score(expected, predicted))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Food ===> Rosianah\n",
      "\n",
      "Machine Learning ===> Rosianah\n",
      "\n",
      "Nigga ===> Lewis\n",
      "\n",
      "Lol ===> Lewis\n",
      "\n",
      "I love you ===> Lewis\n",
      "\n",
      "Go to hell ===> Lewis\n",
      "\n",
      "Yaaay ===> Lewis\n",
      "\n",
      "Nice ===> Lewis\n"
     ]
    }
   ],
   "source": [
    "#Predicting Outcome\n",
    "tweet1 = 'Food'\n",
    "tweet2 = 'Machine Learning'\n",
    "tweet3 = 'Nigga'\n",
    "tweet4 = 'Lol'\n",
    "tweet5 = 'I love you'\n",
    "tweet6 = 'Go to hell'\n",
    "tweet7 = 'Yaaay'\n",
    "tweet8 = 'Nice'\n",
    "\n",
    "tweets_new = [tweet1, tweet2, tweet3, tweet4,tweet5,tweet6, tweet7, tweet8]\n",
    "X_new_counts = count_vect.transform(tweets_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for tw, category in zip(tweets_new, predicted):\n",
    "    print('\\n{} ===> {}'.format(tw, target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
